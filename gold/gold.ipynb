{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72828c63-a40e-4a30-abf5-dbe272be6ebf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, avg, sum as spark_sum, countDistinct, round\n",
    "\n",
    "spark = SparkSession.builder.appName(\"GoldPatientReadmission\").getOrCreate()\n",
    "\n",
    "# -------------------------\n",
    "# Paths\n",
    "# -------------------------\n",
    "silver_fact_path = \"/mnt/silver/fact_visit\"\n",
    "silver_patient_path = \"/mnt/silver/dim_patient\"\n",
    "silver_hospital_path = \"/mnt/silver/dim_hospital\"\n",
    "silver_diag_path = \"/mnt/silver/dim_diagnosis\"\n",
    "gold_path = \"/mnt/gold/gold_patient_readmission\"\n",
    "checkpoint_path = \"/mnt/chk/gold_patient_readmission\"\n",
    "\n",
    "# -------------------------\n",
    "# Read Silver Tables\n",
    "# -------------------------\n",
    "df_fact = spark.read.format(\"delta\").load(silver_fact_path)\n",
    "df_patient = spark.read.format(\"delta\").load(silver_patient_path)\n",
    "df_hospital = spark.read.format(\"delta\").load(silver_hospital_path)\n",
    "df_diag = spark.read.format(\"delta\").load(silver_diag_path)\n",
    "\n",
    "# -------------------------\n",
    "# Join Fact with Dimensions\n",
    "# -------------------------\n",
    "df_gold_base = (\n",
    "    df_fact\n",
    "        .join(df_patient.select(\"patient_sk\", \"patient_id_masked\", \"age_group\", \"gender\"), \"patient_sk\", \"left\")\n",
    "        .join(df_hospital.select(\"hospital_sk\", \"hospital_name\", \"city\", \"state\"), \"hospital_sk\", \"left\")\n",
    "        .join(df_diag.select(\"diagnosis_sk\", \"diagnosis_name\", \"risk_category\"), \"diagnosis_sk\", \"left\")\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Derive KPIs\n",
    "# -------------------------\n",
    "df_gold = (\n",
    "    df_gold_base\n",
    "        .withColumn(\"was_readmitted\", when(col(\"is_readmission_30d\") == 1, \"Yes\").otherwise(\"No\"))\n",
    "        # Simple risk score example\n",
    "        .withColumn(\"risk_score\",\n",
    "            (when(col(\"age_group\") == \"65+\", 2).otherwise(0) +\n",
    "             when(col(\"days_since_last_discharge\") <= 30, 3).otherwise(0) +\n",
    "             when(col(\"risk_category\") == \"High\", 3).otherwise(0))\n",
    "        )\n",
    "        .withColumn(\"load_timestamp\", col(\"load_timestamp\"))\n",
    "        .select(\n",
    "            \"patient_id_masked\",\n",
    "            \"age_group\",\n",
    "            \"gender\",\n",
    "            \"hospital_name\",\n",
    "            \"city\",\n",
    "            \"state\",\n",
    "            \"diagnosis_name\",\n",
    "            \"admission_date\",\n",
    "            \"discharge_date\",\n",
    "            \"prev_discharge\",\n",
    "            \"days_since_last_discharge\",\n",
    "            \"was_readmitted\",\n",
    "            \"risk_score\",\n",
    "            \"cost\",\n",
    "            \"load_timestamp\"\n",
    "        )\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Merge into Gold Table Incrementally\n",
    "# -------------------------\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "def merge_gold_patient(batch_df, batch_id):\n",
    "    target = gold_path\n",
    "\n",
    "    if not DeltaTable.isDeltaTable(spark, target):\n",
    "        batch_df.write.format(\"delta\").mode(\"overwrite\").save(target)\n",
    "        return\n",
    "\n",
    "    gold = DeltaTable.forPath(spark, target)\n",
    "\n",
    "    gold.alias(\"t\").merge(\n",
    "        batch_df.alias(\"s\"),\n",
    "        \"t.patient_id_masked = s.patient_id_masked AND t.admission_date = s.admission_date\"\n",
    "    ) \\\n",
    "    .whenMatchedUpdateAll() \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()\n",
    "\n",
    "# -------------------------\n",
    "# Streaming Write (availableNow)\n",
    "# -------------------------\n",
    "(\n",
    "    df_gold.writeStream\n",
    "        .foreachBatch(merge_gold_patient)\n",
    "        .outputMode(\"update\")\n",
    "        .trigger(availableNow=True)\n",
    "        .option(\"checkpointLocation\", checkpoint_path)\n",
    "        .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75b95f86-4e8c-4915-9abf-0a6554f201a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Optimized gold layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60c0ecd3-bbd0-45a1-a54a-2d1a3bfbb939",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.streaming import DataStreamWriter\n",
    "\n",
    "spark = SparkSession.builder.appName(\"GoldSimple\").getOrCreate()\n",
    "\n",
    "silver_fact_path = \"/mnt/silver/fact_visit\"\n",
    "gold_path = \"/mnt/gold/gold_patient_readmission\"\n",
    "checkpoint = \"/mnt/chk/gold_patient_readmission\"\n",
    "\n",
    "# Read silver fact\n",
    "df_fact = spark.read.format(\"delta\").load(silver_fact_path)\n",
    "\n",
    "# Create Gold table with KPIs\n",
    "df_gold = (\n",
    "    df_fact\n",
    "    .withColumn(\"was_readmitted\", when(col(\"is_readmission_30d\") == 1, \"Yes\").otherwise(\"No\"))\n",
    "    .withColumn(\"risk_score\",\n",
    "        (when(col(\"age_group\") == \"65+\", 2).otherwise(0) +\n",
    "         when(col(\"days_since_last_discharge\") <= 30, 3).otherwise(0) +\n",
    "         when(col(\"risk_category\") == \"High\", 3).otherwise(0))\n",
    "    )\n",
    "    .select(\n",
    "        \"patient_id_masked\",\n",
    "        \"age_group\",\n",
    "        \"gender\",\n",
    "        \"hospital_name\",\n",
    "        \"city\",\n",
    "        \"diagnosis_name\",\n",
    "        \"admission_date\",\n",
    "        \"discharge_date\",\n",
    "        \"prev_discharge\",\n",
    "        \"days_since_last_discharge\",\n",
    "        \"was_readmitted\",\n",
    "        \"risk_score\",\n",
    "        \"cost\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Incremental MERGE logic\n",
    "def merge_gold(batch_df, batch_id):\n",
    "    if not DeltaTable.isDeltaTable(spark, gold_path):\n",
    "        batch_df.write.format(\"delta\").mode(\"overwrite\").save(gold_path)\n",
    "    else:\n",
    "        gold = DeltaTable.forPath(spark, gold_path)\n",
    "        gold.alias(\"t\").merge(\n",
    "            batch_df.alias(\"s\"),\n",
    "            \"t.patient_id_masked = s.patient_id_masked AND t.admission_date = s.admission_date\"\n",
    "        ).whenMatchedUpdateAll()\\\n",
    "         .whenNotMatchedInsertAll()\\\n",
    "         .execute()\n",
    "\n",
    "# Streaming write (available now, single/multiple files incremental)\n",
    "df_gold.writeStream.foreachBatch(merge_gold)\\\n",
    "    .outputMode(\"update\")\\\n",
    "    .trigger(availableNow=True)\\\n",
    "    .option(\"checkpointLocation\", checkpoint)\\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f074c065-4d8e-4930-ba3b-ca93477cb634",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
